{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Runs my implementation of Word2Vec\n",
    "## The input is a text file plus some parameters \n",
    "## The pre-processing is performed by functions in TexttoDict\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import shared\n",
    "import nltk\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from TexttoDict import *\n",
    "\n",
    "rng = np.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------\n",
    "# C is the size of the bag of words\n",
    "# N is the size of hidden layer\n",
    "# V is length of dictcut\n",
    "\n",
    "class NNet(object):\n",
    "    def __init__(self,V,N):\n",
    "\n",
    "        # inititalizing the weight matrices    \n",
    "        self.W1 = theano.shared(rng.randn(V,N), name=\"W1\")   # a V x N matrix\n",
    "        self.W2 = theano.shared(rng.randn(V,N), name=\"W2\")   # a V x N matrix\n",
    "\n",
    "# (x,y) are shared variables for (input, output)\n",
    "# x is the input bag of words\n",
    "# y is the target word, the middle word in the bag\n",
    "        x = T.ivector(\"x\") # a vector\n",
    "        y = T.ivector(\"y\") # a vector\n",
    "\n",
    "# This evaluates the hidden and output layers\n",
    "# This is a linear operation, no logistic function unlike a typical neural net\n",
    "# To be more precise we are evaluating these at the level of the hidden layer\n",
    "        vhidden = self.W1[x] # a C x N matrix\n",
    "        vout = self.W2[y] # a 1 x N matrix\n",
    "\n",
    "        vhiddenvec= T.mean(vhidden,axis=0)  # an N-vector\n",
    "        voutvec=vout.reshape((N,)) # an N-vector\n",
    "    \n",
    "# Entropy loss function (a scalar)\n",
    "        Entropy = -T.dot(vhiddenvec,voutvec) + T.log(T.exp(T.dot(vhidden,self.W2.T)).sum())\n",
    "\n",
    "# Compute the gradient of the cost wrt the weights\n",
    "# This is the computational power of Theano\n",
    "        gradW1, gradW2 = T.grad(Entropy, [self.W1, self.W2])\n",
    "\n",
    "# The train function updates the weights and computes the entropy\n",
    "        train = theano.function(\n",
    "              inputs=[x,y],\n",
    "              outputs=Entropy,\n",
    "              updates=((self.W1, self.W1 - 0.1 * gradW1),(self.W2, self.W2 - 0.1 * gradW2)))\n",
    "    \n",
    "        self.train=train\n",
    "\n",
    "#---------------------------------------\n",
    "    def NNettrain(self,inarr, outarr):\n",
    "        for i in range(len(inarr)):\n",
    "            ent=self.train(inarr[i], outarr[i])\n",
    "    \n",
    "        return ent\n",
    "    \n",
    "    def returnweights(self):\n",
    "        return self.W1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides the bags of words from the textfile\n",
    "using functions from TexttoDict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2Vdata=w2vdatafromtext(\"delorme.txt\", contsize=7, trainlines=10,freqcutoff=1)\n",
    "V=len(W2Vdata[0])\n",
    "N=10\n",
    "MyNet=NNet(V,N)\n",
    "\n",
    "Error=MyNet.NNettrain(W2Vdata[1], W2Vdata[2])\n",
    "weights= MyNet.returnweights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.51182813175\n",
      "[[ 0.64028604 -0.26418584  1.14328397 -0.56986481 -1.78292628 -0.28601964\n",
      "  -0.60396161 -0.2136415   0.42498458  0.17223576]\n",
      " [-1.66858568  1.15064708 -0.05522523  1.22648787 -0.82812032 -0.04576207\n",
      "   0.2634071   0.26448961 -1.58989639 -0.78404094]\n",
      " [ 0.66965756 -0.43303661  2.30687201 -0.53812629 -0.99528095 -0.41631557\n",
      "   0.72584635 -0.11530394  0.21781397 -0.09979045]\n",
      " [-1.31546278  0.9467746   0.24732813 -0.20097082 -1.36930517 -1.32373221\n",
      "  -0.51406696  0.91105362 -0.63977714 -0.12562638]\n",
      " [-1.9656617   1.69242354 -0.52846124  0.10058404 -1.00056113 -0.86589279\n",
      "   0.91224405  0.67518977  1.01358875 -1.14054078]\n",
      " [ 0.78470163  0.20007087 -2.20196821 -1.38329418 -0.2046402  -1.27394646\n",
      "   0.9059332   0.03191097 -0.84300659 -0.02023273]]\n"
     ]
    }
   ],
   "source": [
    "print Error\n",
    "print weights.get_value()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
