{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Runs my implementation of Word2Vec\n",
    "## The input is a text file plus some parameters \n",
    "## The pre-processing is performed by functions in TexttoDict\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import shared\n",
    "import nltk\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from TexttoDict import *\n",
    "\n",
    "rng = np.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------\n",
    "def initweights(V,N):\n",
    "    W1 = theano.shared(rng.randn(V,N), name=\"W1\")   # a V x N matrix\n",
    "    W2 = theano.shared(rng.randn(V,N), name=\"W2\")   # a V x N matrix\n",
    "    return [W1,W2]\n",
    "\n",
    "#---------------------------------------\n",
    "# C is the size of the bag of words\n",
    "def gettrain(W1,W2):\n",
    "    \n",
    "# N is the size of hidden layer\n",
    "# V is length of dictcut\n",
    "    V=len(W1.get_value())\n",
    "    N=len(W1.get_value()[0])\n",
    "\n",
    "# (x,y) are shared variables for (input, output)\n",
    "# x is the input bag of words\n",
    "# y is the target word, the middle word in the bag\n",
    "    x = T.ivector(\"x\") # a vector\n",
    "    y = T.ivector(\"y\") # a vector\n",
    "\n",
    "# This evaluates the hidden and output layers\n",
    "# This is a linear operation, no logistic function unlike neural nets\n",
    "# To be more precise we are evaluating these at the level of the hidden layer\n",
    "    vhidden = W1[x] # a C x N matrix\n",
    "    vout = W2[y] # a 1 x N matrix\n",
    "\n",
    "    vhiddenvec= T.mean(vhidden,axis=0)  # an N-vector\n",
    "    voutvec=vout.reshape((N,)) # an N-vector\n",
    "    \n",
    "# Entropy loss function (this is a scalar)\n",
    "    Entropy = -T.dot(vhiddenvec,voutvec) + T.log(T.exp(T.dot(vhidden,W2.T)).sum())\n",
    "\n",
    "# Compute the gradient of the cost wrt the weights\n",
    "    gradW1, gradW2 = T.grad(Entropy, [W1, W2])\n",
    "\n",
    "# The train function updates the weights and computes the entropy\n",
    "    train = theano.function(\n",
    "          inputs=[x,y],\n",
    "          outputs=[W1,W2,Entropy],\n",
    "          updates=((W1, W1 - 0.1 * gradW1),(W2, W2 - 0.1 * gradW2)))\n",
    "   \n",
    "    return train\n",
    "\n",
    "#---------------------------------------\n",
    "def W2Vtrain(inarr,outarr):\n",
    "    for i in range(len(inarr)):\n",
    "        W1,W2,ent=train(inarr[i], outarr[i])\n",
    "    \n",
    "    return [W1, ent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides the bags of words from the textfile\n",
    "using functions from TexttoDict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2Vdata=w2vdatafromtext(\"delorme.txt\", contsize=7, trainlines=50000,freqcutoff=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the functions in this file to prepare for W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V=len(W2Vdata[0])\n",
    "N=10\n",
    "weights=initweights(V,N)\n",
    "train=gettrain(weights[0],weights[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run W2V on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.31743458,  1.13247936, -0.32688617, ...,  1.01424284,\n",
       "         -1.51509633,  0.0956106 ],\n",
       "        [ 0.74059688, -0.09327022, -0.71152648, ..., -1.29530879,\n",
       "         -0.56048464, -1.20894557],\n",
       "        [ 1.80307889,  0.73352653, -0.09453358, ..., -0.19182782,\n",
       "         -0.19537542,  0.34890444],\n",
       "        ..., \n",
       "        [ 0.54006604,  0.1307789 ,  0.58469422, ...,  0.43557733,\n",
       "         -0.0661391 ,  0.60073479],\n",
       "        [-0.69574132,  0.43231904,  0.54627533, ..., -0.44183667,\n",
       "         -0.22953882, -1.58285563],\n",
       "        [ 0.5538179 , -1.61121227, -0.24933756, ...,  0.20676074,\n",
       "          0.96138345,  0.15373191]]), array(8.694616360398802)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2Vtrain(W2Vdata[1],W2Vdata[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
